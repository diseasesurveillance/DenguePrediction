% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/compare_Measurement.R
\name{compare_Measurement}
\alias{compare_Measurement}
\title{Compare Prediction Performance Across Models}
\usage{
compare_Measurement(data, relative_to_naive = TRUE)
}
\arguments{
\item{data}{A data frame where the first column contains actual values and the subsequent 
columns contain predictions from different models. Column names should reflect model names.}

\item{relative_to_naive}{A logical flag indicating whether to compute performance metrics relative 
to those of a "Naive" model, which is assumed to be the last column in `data`.
If `TRUE`, metrics for each model are divided by the corresponding metric 
of the Naive model to assess performance relative to this baseline. 
Defaults to `TRUE`.}
}
\value{
A data frame containing performance metrics (RMSE, MAE, RMSPE, MAPE, and CORR) for each model. 
        If `relative_to_naive` is TRUE, these metrics are presented relative to the Naive model's performance. 
        The data frame includes model names and rounded metrics for clearer comparison.
}
\description{
Evaluates the performance of various predictive models by calculating performance metrics 
for each model's forecasts compared to actual data. Optionally, it also compares each model's 
performance relative to a baseline "Naive" model. The function expects a data frame where the 
first column contains the actual values, and subsequent columns contain predicted values from 
different models. The function leverages `get_Measurement` for calculating metrics such as RMSE, MAE, etc.
}
\examples{
# Assuming `model_predictions` is your data frame with actual values in the first column and
# predictions from various models in subsequent columns
compare_metrics <- compare_Measurement(model_predictions, relative_to_naive = TRUE)
}
